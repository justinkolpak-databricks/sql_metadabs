from databricks.bundles.jobs import Job, notebook_task, sql_notebook_task, task
import json

# Open json metadata and deserialize to dict
with open('task_metadata.json') as f:
    task_list = json.load(f)

@sql_notebook_task(
    notebook_path='notebooks/bronze_ingest.sql', warehouse_id='475b94ddc7cd5211'
)
def ingestion_task(
    src_catalog: str,
    src_schema: str,
    src_table: str,
    tgt_catalog: str,
    tgt_schema: str,
    tgt_table: str,
):
    pass

job_with_autogenerated_tasks = Job.create(
   resource_name='sql_metadabs_ingest',
   name='sql_metadabs_ingest',
   tasks=[
       ingestion_task(
           src_catalog=task['src_catalog'],
           src_schema=task['src_schema'],
           src_table=task['src_table'],
           tgt_catalog=task['tgt_catalog'],
           tgt_schema=task['tgt_schema'],
           tgt_table=task['tgt_table'],
       ).with_task_key(
           f'load_{task['tgt_table']}'
       )
       for task in task_list if task['batch'] == 'bronze_01'
   ]
)